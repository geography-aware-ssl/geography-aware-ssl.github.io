<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Novel training methods that exploit the spatio-temporal structure of remote sensing data.">
  <meta name="keywords" content="satellite imagery, remote sensing, self-supervised learning, representation learning, geo-location, object detection, classification, segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Geography-Aware Self-Supervised Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->


  <script>
    var doc = document.querySelector('link[rel="import"]').import;
    // Grab DOM from doc.html's document.
    var text = doc.querySelector('.doc');
    document.body.appendChild(text.cloneNode(true));
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    th, td {

    text-align: left;

    vertical-align: middle;

    border-collapse: collapse;

    border: 0.5px solid black;

    padding: 8px;

  }
  </style>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Geography-Aware Self-Supervised Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="index.html">Kumar Ayush*</a>,</span>
            <span class="author-block">
              <a href="https://uzkent.github.io/">Burak Uzkent*</a>,</span>
            <span class="author-block">
              <a href="https://cs.stanford.edu/~chenlin/">Chenlin Meng</a>,</span>
            <span class="author-block">
              <a href="index.html">Kumar Tanmay</a>,</span>
            <span class="author-block">
              <a href="https://web.stanford.edu/~mburke/">Marshall Burke</a>,
            </span>
            <span class="author-block">
              <a href="https://earth.stanford.edu/people/david-lobell">David B. Lobell</a>,
            </span>
            <span class="author-block">
              <a href="http://cs.stanford.edu/~ermon/">Stefano Ermon</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Stanford University</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://iccv2021.thecvf.com/home">ICCV 2021</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.09980.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.09980"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<center>
<img id="teaser" src="./static/images/Unknown-2.png" width="30%">
</center>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/time_series_data.png" height="100%">
      <!-- <video id="teaser" autoplay muted loop height="100%">
        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        Images over time concept in the <a href="https://www.iarpa.gov/challenges/fmow.html">Functional Map of the World</a> (fMoW) dataset. The metadata associated with each image is shown underneath.
        We can see changes in contrast, brightness, cloud cover etc. in the images. These changes render spatially aligned images over time useful for constructing additional positives.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop height="100%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Contrastive  learning  methods  have  significantly  narrowed the gap between supervised and unsupervised learning on computer vision tasks. In this paper, we explore their application  to  geo-located  datasets, e.g. remote  sensing, where  unlabeled  data  is  often  abundant  but  labeled  data is  scarce.   We  first  show  that  due  to  their  different  characteristics,  a  non-trivial  gap  persists  between  contrastive and supervised learning on standard benchmarks. To close the gap, we propose novel training methods that exploit the spatio-temporal structure of remote sensing data. We leverage spatially aligned images over time to construct temporal positive pairs in contrastive learning and geo-location to  design  pre-text  tasks. Our  experiments  show  that  our proposed  method  closes  the  gap  between  contrastive  and supervised learning on image classification,  object detection and semantic segmentation for remote sensing.  Moreover, we demonstrate that the proposed method can also beapplied to geo-tagged ImageNet images, improving down-stream performance on various tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Model</h2>
          <center><img id="model" src="./static/images/ap1.png" style="width:50%"></center>
          <br>
          <center><img id="model" src="./static/images/ap2.png" style="width:50%"></center>
          <br>
          <h2 class="subtitle has-text-centered">
            <b>Top</b> shows the original <a href="https://arxiv.org/abs/2003.04297">MoCo-v2</a> framework. <b>Bottom</b> shows the schematic overview of our approach.
          </h2>
        <div class="content has-text-justified">
          By leveraging spatially aligned images over time to construct temporal positive pairs in contrastive learning and geo-location in the design of pre-text tasks, we are able to close the gap between self-supervised and supervised learning on image classification, object detection and semantic segmentation on remote sensing and other geo-tagged image datasets.
          <br><br>
          Our experiments on the <a href="https://www.iarpa.gov/challenges/fmow.html">Functional Map of the World</a> (fMoW) dataset consisting of high spatial resolution satellite images show that we improve <a href="https://arxiv.org/abs/2003.04297">MoCo-v2</a> baseline significantly. In particular, we improve it by <span>&#126;</span>8% classification accuracy when testing the learned representations on image classification, <span>&#126;</span>2% AP on object detection, <span>&#126;</span>1% mIoU on semantic segmentation, and <span>&#126;</span>3% top-1 accuracy on land cover classification. Interestingly, our geography-aware learning can even outperform the supervised learning counterpart on temporal data classification by <span>&#126;</span>2%. With the proposed method, we can improve the accuracy on target applications utilizing object detection, and semantic segmentation.
        </div>
      </div>
    </div>
    <!--/ Method. -->


    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">GeoImageNet</h2>
        <img id="model" src="./static/images/geoImagenet.png" height="100%">
        <h2 class="subtitle has-text-centered">
            Some examples from GeoImageNet dataset.  Below each image, we list their latitudes, longitudes, city, country name.  In our study, we use the latitude and longitude information for unsupervised learning.
        </h2>
        <div class="content has-text-justified">
            To further demonstrate the effectiveness of our geography-aware learning approach, we  searched  for  geo-tagged  images in  ImageNet  using  the  FLICKR  API,  and  were  able  to find  543,435  images  with  their  associated  coordinates (lat<sub>i</sub>, lon<sub>i</sub>) across  5150  class  categories.   This  dataset  is more challenging than ImageNet-1K as it is highly imbalanced and contains about 5X more classes. We extend the proposed approaches to geo-located ImageNet, and show that geography-aware learning can improve the performance of <a href="https://arxiv.org/abs/2003.04297">MoCo-v2</a> by <span>&#126;</span>2% on image classification, showing the potential application of our approach  to  any  geo-tagged  dataset. 
            <br><br>
            We show some examples from GeoImageNet in the figure above, for  some  images  we  have  geo-coordinates that can be predicted from visual cues.  For example, we see that a picture of a person with a Sombrerohat was captured in Mexico.  Similarly, an Indian Elephant picture was captured in India, where there is a large population of Indian Elephants. Next to it, we show the picture of an African Elephant (which is larger in size).  If a model is trained to predict where in the world the image was taken,it should be able to identify visual cues that are transferable to other tasks (e.g., visual cues to differentiate Indian Ele-phants from the African counterparts).  
        </div>
        <center><img id="model" src="./static/images/fmow_coords.png" height="50%"></center>
        <h2 class="subtitle has-text-centered">
            Shows the distribution of the fMoW
        </h2>
        <center><img id="model" src="./static/images/geoin_coords.png" height="50%"></center>
        <h2 class="subtitle has-text-centered">
            Shows the distribution of GeoImageNet
        </h2>
      </div>
    </div>
    <!--/ Results. -->

  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/ICCV 2021 Poster.png" height="100%">
      <!-- <video id="teaser" autoplay muted loop height="100%">
        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        Poster
      </h2>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ayush2021geography,
      title={Geography-Aware Self-Supervised Learning},
      author={Ayush, Kumar and Uzkent, Burak and Meng, Chenlin and Tanmay, Kumar and Burke, Marshall and Lobell, David and Ermon, Stefano},
      journal={ICCV},
      year={2021}
    }
</code></pre>
  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work</h2>
          <div class="content has-text-justified">
            Previous work has used high resolution satellite imagery for 
            predicting economic development <a href="https://arxiv.org/abs/2002.01612">[Ayush et al., 2020]</a>. Low resolution satellite imagery has also been used for this task <a href="https://arxiv.org/abs/1711.03654">[Perez et al., 2017]</a>. Previous work in fusion models for satellite imagery include <a href="https://ieeexplore.ieee.org/document/1661809">[Gao et al., 2006]</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0034425710001884">[Zhu et al., 2010]</a>, and <a href="https://arxiv.org/abs/2011.04762">[Bouabid et al., 2020]</a>. In contrast to previous work, which were focused on generating lower resolution satellite imagery, we are interested in generating very high resolution images, which are more useful for downstream tasks such as object detection for poverty level estimation.
          </div>
      </div>
    </div>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>The format of the website is borrowed from the <a href="https://nerfies.github.io/">Nerfies</a> project website.</p>
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
